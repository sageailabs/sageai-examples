{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e853db0-b5a5-4786-8a80-fb6e9afb4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from demo_utils import calculate_stats, collect_responses, display_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd7162-1fd7-42b8-a046-4576e47620ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5251fbd-39db-4f45-be9a-53a7a1ed34c3",
   "metadata": {},
   "source": [
    "# Set up model and tokenizer.\n",
    "We'll be using a bare-bones [Mistral model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) for our examples.\n",
    "\n",
    "In practice, you could pass your model weights into the `from_pretrained` function to load\n",
    "weights from fine-tuned or pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df347d-cefe-4b5f-8bc6-aefe64f2986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             trust_remote_code=True, \n",
    "                                             device_map=\"auto\", \n",
    "                                             attn_implementation=\"eager\")\n",
    "model.eval()\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33f026-cc05-4168-9561-c70f1d53b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86c80d-47c7-4bac-b808-93ebff44b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "now = datetime.now(timezone.utc)\n",
    "instruction_template = \"straightforward\"\n",
    "dataset = pd.read_csv(\"./blog_toy_dataset.csv\")\n",
    "output_dataset_path = f\"./base-mistral_{instruction_template}_{now.year}-{now.month}-{now.day}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424ca06-b54b-4b8d-af11-33b6a81b1580",
   "metadata": {},
   "source": [
    "# Run evaluation on our first instruction template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c93c68-7aa8-4306-b6a8-73e4e9e5f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_responses(dataset=dataset, \n",
    "                  model=model, \n",
    "                  instruction_template=instruction_template, \n",
    "                  tokenizer=tokenizer, \n",
    "                  debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6d9fb-2843-4c88-92bb-1a0e5252a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_stats = calculate_stats(dataset=dataset)\n",
    "display_stats(stats_dict=evaluation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ada679-bae4-479d-ad8b-0bc460f448b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(output_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ee72b-a7dd-484b-90ad-477669485c1e",
   "metadata": {},
   "source": [
    "# Run evaluation on the `just_answer` instruction template.\n",
    "First, we'll get a clean dataset since our last run appended its results inplace.\n",
    "\n",
    "In practice, the user would want to parallelize these three experiments or run them in separate notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a21c91-d1f5-457a-a8d5-769cf3a5723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_template = \"just_answer\"\n",
    "dataset = pd.read_csv(\"./blog_toy_dataset.csv\")\n",
    "output_dataset_path = f\"./base-mistral_{instruction_template}_{now.year}-{now.month}-{now.day}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce68a0e-8de9-4ea9-b4b0-65e3b9441e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_responses(dataset=dataset, \n",
    "                  model=model, \n",
    "                  instruction_template=instruction_template, \n",
    "                  tokenizer=tokenizer, \n",
    "                  debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcb697-509b-4d35-9a41-6d5b3689448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_stats = calculate_stats(dataset=dataset)\n",
    "display_stats(stats_dict=evaluation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739119d-933c-4221-a640-cdc37da6c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(output_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead868e0-330e-4331-b787-c892b8facfa8",
   "metadata": {},
   "source": [
    "# Run evaluation on our final instruction template.\n",
    "Get a new, cleaned dataset and start a new evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43a608-f47e-47e4-a957-ef4439f0c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_template = \"accounting_related\"\n",
    "dataset = pd.read_csv(\"./blog_toy_dataset.csv\")\n",
    "output_dataset_path = f\"./base-mistral_{instruction_template}_{now.year}-{now.month}-{now.day}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5284a-a2b0-465a-b566-0e94aee1c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_responses(dataset=dataset, \n",
    "                  model=model, \n",
    "                  instruction_template=instruction_template, \n",
    "                  tokenizer=tokenizer, \n",
    "                  debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c89c6-90d2-43ef-ab6e-df5cc88bf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_stats = calculate_stats(dataset=dataset)\n",
    "display_stats(stats_dict=evaluation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39afe6-5adc-4568-b50f-1580075d2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(output_dataset_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-llm_evaluation_demo]",
   "language": "python",
   "name": "conda-env-.conda-llm_evaluation_demo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
